{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Grundlegendes Funktionsprinzip neronaler Netze \n",
    "\n",
    "+ **Biologische Inspiration:** Künstliche neuronale Netze (KNN) orientieren sich am biologischen Gehirnen, insbesondere an der Funktionsweise von Neuronen.\n",
    "\n",
    "+ Ein Neuron feuert ein Signal nur dann, wenn eine bestimmte Eingabebeschwellung überschritten wird. \n",
    "\n",
    "+ **Künstliche Neuronen:** summiert mehrere Eingaben, gewichtet diese und verwendet eine Aktivierungsfunktion (z.B. Sigmoid), um den Ausgang zu berehnen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Architektur neuronale Netze \n",
    "\n",
    "+ **Schichten:** KNN bestehen aus mehreren Schichten, einer Eingabeschicht, eine oder mehrere versteckte Schichten und einer Ausgabeschicht. Jede Schicht transformiert die Eingabesignale und gibt sie an die nächste Schicht.\n",
    "\n",
    "+ **Gewichte:** Die Verbindungen zwischen Neuronen sind durch Gewichte charakterisiert. Diese Gewichte bestimmen die Stärke des Signals und während des Lernens angepasst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Lernprozess \n",
    "\n",
    "+ **Gewichtsveränderungen:** Das Lenrnen erfolgt durch Anpassung der Gewichte, um den Fehler zwischen der tätsächlichen und der gewünschten Ausgaben zu minimiren. \n",
    "\n",
    "+ **Backpropagation:** Ist ein gängiger Algorithmus zur Fehlerverringerung. Dabei wird der Fehler von der ausgabeschicht bis zur Eingabeschihct aufsummiert, also propagiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Mathematisch Darstellung \n",
    "\n",
    "+ Matrizenmultiplikation: Der Prozess der Signalweitergabe in einem neuronalen Netz kann als Matrizenmultiplikation beschrieben werden. Die Gesamtmatrix multipliziert mit der Eingangsmatrix ergibt eine Eingabe für die Nächste Schicht. \n",
    "$$ \\boxed{ M = X \\cdot I}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Aktivierungsfunktion \n",
    "\n",
    "+ **Stufenfunktion:** Gibt ein Signal nur weiter, wenn ein Wert einen bestimmten Schwellenwert überschreitet\n",
    "\n",
    "+ **Sigmoidfunktion:** Glättet die Ausgabe zwischen 0 und 1, wodruch kleinere Änderungen in den Eingaben kontinuierlich Änderungen in der Ausgabe bewirken.\n",
    "\n",
    "$$ \\boxed{sigmoid(x) = \\frac{1}{1+e^{-x}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Grundlagen des Lernens neurales Netz \n",
    "\n",
    "+ Neuronale Netze lernen durch Anpassung der Verknüpfungsgewichte. Dies erfolgt auf Basis des Fehlers, der als Differenz zwischen Soll- und Ist-Ausgabe definiert ist.\n",
    "\n",
    "+ Der Lernprozess umfasst die Rückfühung des Fehlers von der Ausgabeschicht über die versteckten Schichten bis zur Eingabeschicht (Backpropagation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Fehlerfunktion und Gradientenverfahren \n",
    "\n",
    "+ Die fehlfunktion misst die Abweichung zwischen den Ausgaben des Netzes un den gewünschten Werten.\n",
    "\n",
    "+ Das Gradientenverfahren wird eingestezt, um das Minimum der Fehlerfunktion zu finden. Es ermittelt die Richtung in diese die Gewichte angepasst werden müssen. \n",
    "\n",
    "+ Es gibt einen Trick beim **Gradientenverfahren**, um das Überschwingen zu reduzieren und trozdem Schnell zu sein, um das Minimum zu finden. Man beginnt mit einer Hohen **Lernrate $\\eta$** um schnell voran zu kommen, sobald sich das vorzeichen ändert halbiert man die **Lernrate $\\eta$** und geht in die andere Richtung. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Mein Bild](Thumbnail-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Gewichtsaktualisierung\n",
    "\n",
    "+ Die gewichte werden entsprechend der berechneten Fehlergradienten angepasst. Eine kleine Lernrate (Schrittweite) verhindert dabei ein Überschießen des Minimums.\n",
    "\n",
    "+ Ein Beispiel um ein Gewicht zu aktualisieren:\n",
    "$$ \\boxed {\\Delta \\omega_{ij} = -\\eta \\cdot Fehler \\cdot Gradient} $$ \n",
    "\n",
    "wobei $\\eta$ die Lernrate ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Backpropagation mit Matrizen\n",
    "\n",
    "+ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
